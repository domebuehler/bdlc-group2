{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "421f5d5a",
   "metadata": {},
   "source": [
    "# Prototyping - Preprocessing - Dataflow\n",
    "### Initialize PySpark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad7d9a6-0e03-4ae8-97ad-9b7df5313cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import findspark\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.appName(\"CSV to Parquet\").config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f01d34",
   "metadata": {},
   "source": [
    "### Get all CSV files from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c84b3-a3b7-4c49-a5da-af4ddc6b0101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRawCsvDataFiles():\n",
    "    import subprocess\n",
    "    p = subprocess.Popen(\"hdfs dfs -ls -d /parkingviolations/rawdata/* | awk '{print $8}' \",\n",
    "                         shell=True,\n",
    "                         stdout=subprocess.PIPE,\n",
    "                         stderr=subprocess.STDOUT)\n",
    "    \n",
    "    csv_files = []\n",
    "    \n",
    "    for line in p.stdout.readlines():\n",
    "        csv_files.append(line.decode().strip())\n",
    "    \n",
    "    p.wait()\n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e95ba-9beb-4a15-8d4d-72d8f68669da",
   "metadata": {},
   "source": [
    "### Define Parquet schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e0ea61b-5a21-481e-883d-40ec0bf64a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "def getSchema():\n",
    "    return StructType([\n",
    "        StructField(\"summons_number\", IntegerType()),\n",
    "        StructField(\"plate_id\", StringType()),\n",
    "        StructField(\"registration_state\", StringType()),\n",
    "        StructField(\"plate_type\", StringType()),\n",
    "        StructField(\"issue_date\", DateType()),\n",
    "        StructField(\"violation_code\", IntegerType()),\n",
    "        StructField(\"vehicle_body_type\", StringType()),\n",
    "        StructField(\"vehicle_make\", StringType()),\n",
    "        StructField(\"issuing_agency\", StringType()),\n",
    "        StructField(\"street_code1\", IntegerType()),\n",
    "        StructField(\"street_code2\", IntegerType()),\n",
    "        StructField(\"street_code3\", IntegerType()),\n",
    "        StructField(\"vehicle_expiration_date\", IntegerType()),\n",
    "        StructField(\"violation_location\", StringType()),\n",
    "        StructField(\"violation_precinct\", IntegerType()),\n",
    "        StructField(\"issuer_precinct\", IntegerType()),\n",
    "        StructField(\"issuer_code\", IntegerType()),\n",
    "        StructField(\"issuer_command\", StringType()),\n",
    "        StructField(\"issuer_squad\", StringType()),\n",
    "        StructField(\"violation_time\", StringType()),\n",
    "        StructField(\"time_first_observed\", StringType()),\n",
    "        StructField(\"violation_county\", StringType()),\n",
    "        StructField(\"violation_in_front_of_or_opposite\", StringType()),\n",
    "        StructField(\"house_number\", StringType()),\n",
    "        StructField(\"street_name\", StringType()),\n",
    "        StructField(\"intersecting_street\", StringType()),\n",
    "        StructField(\"date_first_observed\", IntegerType()),\n",
    "        StructField(\"law_section\", IntegerType()),\n",
    "        StructField(\"sub_division\", StringType()),\n",
    "        StructField(\"violation_legal_code\", StringType()),\n",
    "        StructField(\"days_parking_in_effect\", StringType()),\n",
    "        StructField(\"from_hours_in_effect\", StringType()),\n",
    "        StructField(\"to_hours_in_effect\", StringType()),\n",
    "        StructField(\"vehicle_color\", StringType()),\n",
    "        StructField(\"unregistered_vehicle\", StringType()),\n",
    "        StructField(\"vehicle_year\", IntegerType()),\n",
    "        StructField(\"meter_number\", StringType()),\n",
    "        StructField(\"feet_from_curb\", IntegerType()),\n",
    "        StructField(\"violation_post_code\", StringType()),\n",
    "        StructField(\"no_standing_or_stopping_violation\", StringType()),\n",
    "        StructField(\"hydrant_violation\", StringType()),\n",
    "        StructField(\"double_parking_violation\", StringType())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b734b6-4910-490f-81c2-6e3c0317a80f",
   "metadata": {},
   "source": [
    "### Get Further Violation Information\n",
    "\n",
    "[Codes-Mapping.xlsx](https://data.cityofnewyork.us/api/views/pvqr-7yc4/files/7875fa68-3a29-4825-9dfb-63ef30576f9e?download=true&filename=ParkingViolationCodes_January2020.xlsx) contains the violation descriptions and the fine amounts for the areas `manhattan_96th_st_below` and `all_other_areas`. \n",
    "\n",
    "Since it is challenging to determine the exact area where each violation occurred, we decided to use the mean value between the fine amounts for these two areas. \n",
    "\n",
    "The difference in fine amounts between these areas is minimal, so we consider this approach to be an acceptable approximation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad72cfe9-4362-4909-8421-2c93ef23839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_violation_codes_df():\n",
    "    pandas_df = pd.read_excel(\"../1_Data/Codes-Mapping.xlsx\", skiprows=1)\n",
    "    pandas_df.columns = ['violation_code', 'violation_description', 'manhattan_96th_st_below', 'all_other_areas']\n",
    "\n",
    "    pandas_df['fine_amount'] = pandas_df[['manhattan_96th_st_below', 'all_other_areas']].mean(axis=1)\n",
    "    \n",
    "    return spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ebc015-b931-4902-9545-4f55d3444634",
   "metadata": {},
   "source": [
    "## Combine violation time and issue date into one timestamp\n",
    "\n",
    "The column `violation_time` has following format: 1059A, 1145P. \n",
    "The suffix `A` stands for AM and `P` stands for PM.\n",
    "\n",
    "We want to convert this to the clearer format `yyyy-MM-dd HHmm`. After reformatting the entries, we save them to the new column `issue_datetime`.\n",
    "\n",
    "~510'000 entries don't have any violation time. This is about 0.4% of all entries. \n",
    "In the cases, where we don't have any violation time, we set the timestamp to the known date and set the time to midnight. \n",
    "Since this deviation is minor, we accept it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7e5853-d402-458b-8707-e4916d9380ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, lit, when, concat, regexp_extract, lpad\n",
    "\n",
    "pattern = r'^(0[1-9]|1[0-2])([0-5][0-9])[APap]$'\n",
    "\n",
    "def clean_and_combine_timestamp(df):\n",
    "    # Replace broken data with 0000A (Midnight).\n",
    "    df = df.withColumn(\n",
    "        \"cleaned_violation_time\",\n",
    "        when(\n",
    "            regexp_extract(col(\"violation_time\"), pattern, 0) == \"\",\n",
    "            lit(\"0000A\")\n",
    "        ).otherwise(col(\"violation_time\"))\n",
    "    )\n",
    "    \n",
    "    # 12 to 24 Hour format conversion.\n",
    "    df = df.withColumn(\n",
    "        \"formatted_violation_time\",\n",
    "        concat(\n",
    "            col(\"issue_date\").cast(\"string\"),\n",
    "            lit(\" \"),\n",
    "            when(\n",
    "                col(\"cleaned_violation_time\").substr(-1, 1) == \"P\",\n",
    "                (col(\"cleaned_violation_time\").substr(1, 2).cast(\"int\") + 12) % 24\n",
    "            ).otherwise(\n",
    "                lpad(col(\"cleaned_violation_time\").substr(1, 2), 2, \"0\")\n",
    "            ).cast(\"string\"),\n",
    "            col(\"cleaned_violation_time\").substr(3, 2)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Additional Column.\n",
    "    df = df.withColumn(\n",
    "        \"issue_datetime\",\n",
    "        to_timestamp(col(\"formatted_violation_time\"), \"yyyy-MM-dd HHmm\")\n",
    "    )\n",
    "\n",
    "    # Cleanup.\n",
    "    df = df.drop(\"cleaned_violation_time\", \"formatted_violation_time\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d2e61-cacd-41af-a33c-9dd68734cb70",
   "metadata": {},
   "source": [
    "### Transform CSV to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba0700-7a19-45eb-80ce-799fc8c26664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, lit, when, concat, regexp_extract\n",
    "\n",
    "def transform_csv_to_df(csv_file):\n",
    "    schema = getSchema()\n",
    "\n",
    "    # Read csv with schema.\n",
    "    df_raw = spark.read.csv(csv_file, header=True, schema=schema, dateFormat=\"MM/DD/YYYY\")\n",
    "    \n",
    "    # Rename columns to underscore case.\n",
    "    df = df_raw.select([col(col_name).alias(col_name.lower().replace(' ', '_')) for col_name in df_raw.columns])\n",
    "\n",
    "    df = clean_and_combine_timestamp(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675f7418-aa73-446f-b517-d0fa37c3a9fc",
   "metadata": {},
   "source": [
    "### Combine all dataframes into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019dde64-dcfb-4999-98a5-da01d8446e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dfs = []\n",
    "for csvFile in getRawCsvDataFiles():\n",
    "    my_dfs.append(transform_csv_to_df(csvFile))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2a72b6",
   "metadata": {},
   "source": [
    "### Join Further Violation Information\n",
    "\n",
    "Here we use the foreign key `violation_code` to join the fields `violation_description` and `fine_amount` on top of the dataframe with the parking violations.\n",
    "\n",
    "This is a good approach to speed up the further data processing, since the data is directly available without the need of an expensive join operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88a7d0-6eef-408b-8419-b22f455f5306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "meta_df = get_violation_codes_df()\n",
    "df = reduce(DataFrame.unionAll, my_dfs)\n",
    "\n",
    "df = df.join(meta_df, on=\"violation_code\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59629bf4-bf11-477a-9dbe-6ebac607aa11",
   "metadata": {},
   "source": [
    "### Write Parquet to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d4a1d5-c1bd-43e3-bf33-3b6b8c5c78aa",
   "metadata": {},
   "source": [
    "- 1 spark executor = 4 cores\n",
    "- 4 machines x 4 executor x 4 cores = 64 partitions\n",
    "\n",
    "We partition the whole Parquet into 64 partitions. Like this, every core gets its own partition.\n",
    "\n",
    "Partitioning the Parquet file into 64 partitions ensures that each core can process its own partition independently. This improves parallel processing efficiency, reduces data shuffling between cores, and optimizes resource utilization across the cluster. By minimizing data movement and allowing each core to work on a specific subset of the data, we achieve faster data processing and better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29425fb-8d9b-4012-9296-42a871b8527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r /parkingviolations/raw_all.parquet/\n",
    "df.repartition(64).write.parquet(f\"/parkingviolations/raw_all.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bbd402-799f-4a46-a4a9-f99dff051380",
   "metadata": {},
   "source": [
    "## Stop Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de21fcf-5dbc-4810-a641-d1f3082fb0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1062708a37074d70712b695aadee582e0b0b9f95f45576b5521424137d05fec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
