{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "421f5d5a",
   "metadata": {},
   "source": [
    "## Initialize PySpark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad7d9a6-0e03-4ae8-97ad-9b7df5313cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import findspark\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.appName(\"CSV to Parquet\").config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f01d34",
   "metadata": {},
   "source": [
    "## Get all CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c84b3-a3b7-4c49-a5da-af4ddc6b0101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRawCsvDataFiles():\n",
    "    import subprocess\n",
    "    p = subprocess.Popen(\"hdfs dfs -ls -d /parkingviolations/rawdata/* | awk '{print $8}' \",\n",
    "                         shell=True,\n",
    "                         stdout=subprocess.PIPE,\n",
    "                         stderr=subprocess.STDOUT)\n",
    "    \n",
    "    csv_files = []\n",
    "    \n",
    "    for line in p.stdout.readlines():\n",
    "        csv_files.append(line.decode().strip())\n",
    "    \n",
    "    p.wait()\n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e95ba-9beb-4a15-8d4d-72d8f68669da",
   "metadata": {},
   "source": [
    "# Create Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ea61b-5a21-481e-883d-40ec0bf64a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "def getSchema():\n",
    "    return StructType([\n",
    "        StructField(\"summons_number\", IntegerType()),\n",
    "        StructField(\"plate_id\", StringType()),\n",
    "        StructField(\"registration_state\", StringType()),\n",
    "        StructField(\"plate_type\", StringType()),\n",
    "        StructField(\"issue_date\", DateType()),\n",
    "        StructField(\"violation_code\", IntegerType()),\n",
    "        StructField(\"vehicle_body_type\", StringType()),\n",
    "        StructField(\"vehicle_make\", StringType()),\n",
    "        StructField(\"issuing_agency\", StringType()),\n",
    "        StructField(\"street_code1\", IntegerType()),\n",
    "        StructField(\"street_code2\", IntegerType()),\n",
    "        StructField(\"street_code3\", IntegerType()),\n",
    "        StructField(\"vehicle_expiration_date\", IntegerType()),\n",
    "        StructField(\"violation_location\", StringType()),\n",
    "        StructField(\"violation_precinct\", IntegerType()),\n",
    "        StructField(\"issuer_precinct\", IntegerType()),\n",
    "        StructField(\"issuer_code\", IntegerType()),\n",
    "        StructField(\"issuer_command\", StringType()),\n",
    "        StructField(\"issuer_squad\", StringType()),\n",
    "        StructField(\"violation_time\", StringType()),\n",
    "        StructField(\"time_first_observed\", StringType()),\n",
    "        StructField(\"violation_county\", StringType()),\n",
    "        StructField(\"violation_in_front_of_or_opposite\", StringType()),\n",
    "        StructField(\"house_number\", StringType()),\n",
    "        StructField(\"street_name\", StringType()),\n",
    "        StructField(\"intersecting_street\", StringType()),\n",
    "        StructField(\"date_first_observed\", IntegerType()),\n",
    "        StructField(\"law_section\", IntegerType()),\n",
    "        StructField(\"sub_division\", StringType()),\n",
    "        StructField(\"violation_legal_code\", StringType()),\n",
    "        StructField(\"days_parking_in_effect\", StringType()),\n",
    "        StructField(\"from_hours_in_effect\", StringType()),\n",
    "        StructField(\"to_hours_in_effect\", StringType()),\n",
    "        StructField(\"vehicle_color\", StringType()),\n",
    "        StructField(\"unregistered_vehicle\", StringType()),\n",
    "        StructField(\"vehicle_year\", IntegerType()),\n",
    "        StructField(\"meter_number\", StringType()),\n",
    "        StructField(\"feet_from_curb\", IntegerType()),\n",
    "        StructField(\"violation_post_code\", StringType()),\n",
    "#StructField(\"violation_description\", StringType()),\n",
    "        StructField(\"no_standing_or_stopping_violation\", StringType()),\n",
    "        StructField(\"hydrant_violation\", StringType()),\n",
    "        StructField(\"double_parking_violation\", StringType())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b734b6-4910-490f-81c2-6e3c0317a80f",
   "metadata": {},
   "source": [
    "## Extract Excel with Violation Codes with pandas into Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad72cfe9-4362-4909-8421-2c93ef23839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_violation_codes_df():\n",
    "    pandas_df = pd.read_excel(\"../1_Hands-on/Codes-Mapping.xlsx\", skiprows=1)\n",
    "    pandas_df.columns = ['violation_code', 'violation_description', 'manhattan_96th_st_below', 'all_other_areas']\n",
    "    return spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ebc015-b931-4902-9545-4f55d3444634",
   "metadata": {},
   "source": [
    "## Here we combine violation time and issue date into one Timestamp column with the help of regex\n",
    "\n",
    "The column 'violation_time' has following format: 1059A, 1145P. \n",
    "The suffix 'A' stands for AM and 'P' stands for PM.\n",
    "\n",
    "We want to convert this to the clearer format 'yyyy-MM-dd HHmm'. After reformatting the entries, we save them to the new column 'issue_datetime'.\n",
    "\n",
    "~510'000 entries don't have any violation time. This is about 0.4% of all Entries. \n",
    "In the cases, where we don't have any violation time, we set the timestamp to the known date and set the time to midnight. \n",
    "Since this deviation is minor, we accept it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7e5853-d402-458b-8707-e4916d9380ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.12.0 64-bit' requires notebook and jupyter package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, lit, when, concat, regexp_extract, lpad\n",
    "\n",
    "\n",
    "pattern = r'^(0[1-9]|1[0-2])([0-5][0-9])[APap]$'\n",
    "\n",
    "def clean_and_combine_timestamp(df):\n",
    "    # Replace broken data with 0000A (Midnight)\n",
    "    df = df.withColumn(\n",
    "        \"cleaned_violation_time\",\n",
    "        when(\n",
    "            regexp_extract(col(\"violation_time\"), pattern, 0) == \"\",\n",
    "            lit(\"0000A\")\n",
    "        ).otherwise(col(\"violation_time\"))\n",
    "    )\n",
    "    \n",
    "    # 12 to 24 Hour format conversion\n",
    "    df = df.withColumn(\n",
    "        \"formatted_violation_time\",\n",
    "        concat(\n",
    "            col(\"issue_date\").cast(\"string\"),\n",
    "            lit(\" \"),\n",
    "            when(\n",
    "                col(\"cleaned_violation_time\").substr(-1, 1) == \"P\",\n",
    "                (col(\"cleaned_violation_time\").substr(1, 2).cast(\"int\") + 12) % 24\n",
    "            ).otherwise(\n",
    "                lpad(col(\"cleaned_violation_time\").substr(1, 2), 2, \"0\")\n",
    "            ).cast(\"string\"),\n",
    "            col(\"cleaned_violation_time\").substr(3, 2)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Additional Column\n",
    "    df = df.withColumn(\n",
    "        \"issue_datetime\",\n",
    "        to_timestamp(col(\"formatted_violation_time\"), \"yyyy-MM-dd HHmm\")\n",
    "    )\n",
    "\n",
    "    # Cleanup\n",
    "    df = df.drop(\"cleaned_violation_time\", \"formatted_violation_time\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d2e61-cacd-41af-a33c-9dd68734cb70",
   "metadata": {},
   "source": [
    "## Transform function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba0700-7a19-45eb-80ce-799fc8c26664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, lit, when, concat, regexp_extract\n",
    "\n",
    "def transform_csv_to_df(csv_file):\n",
    "    schema = getSchema()\n",
    "\n",
    "    # Read csv with schema\n",
    "    df_raw = spark.read.csv(csv_file, header=True, schema=schema, dateFormat=\"MM/DD/YYYY\")\n",
    "    \n",
    "    # Rename columns to underscore case\n",
    "    df = df_raw.select([col(col_name).alias(col_name.lower().replace(' ', '_')) for col_name in df_raw.columns])\n",
    "\n",
    "    df = clean_and_combine_timestamp(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675f7418-aa73-446f-b517-d0fa37c3a9fc",
   "metadata": {},
   "source": [
    "# Now we combine all dataframes into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019dde64-dcfb-4999-98a5-da01d8446e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dfs = []\n",
    "for csvFile in getRawCsvDataFiles():\n",
    "    my_dfs.append(transform_csv_to_df(csvFile))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2a72b6",
   "metadata": {},
   "source": [
    "Here we join the fees for the violation by the 'violation_code' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88a7d0-6eef-408b-8419-b22f455f5306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "meta_df = get_violation_codes_df()\n",
    "df = reduce(DataFrame.unionAll, my_dfs)\n",
    "\n",
    "df = df.join(meta_df, on=\"violation_code\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59629bf4-bf11-477a-9dbe-6ebac607aa11",
   "metadata": {},
   "source": [
    "## Write results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d4a1d5-c1bd-43e3-bf33-3b6b8c5c78aa",
   "metadata": {},
   "source": [
    "### 1 Core Executor = 4 Cores\n",
    "#### 4 Machines x 4 Core Executor x 4 Cores (BDLC 01-09) = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29425fb-8d9b-4012-9296-42a871b8527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r /parkingviolations/raw_all.parquet/\n",
    "df.repartition(64).write.parquet(f\"/parkingviolations/raw_all.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bbd402-799f-4a46-a4a9-f99dff051380",
   "metadata": {},
   "source": [
    "## Stop Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de21fcf-5dbc-4810-a641-d1f3082fb0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1062708a37074d70712b695aadee582e0b0b9f95f45576b5521424137d05fec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
