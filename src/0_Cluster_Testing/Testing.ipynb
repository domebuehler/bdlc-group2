{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae611828-2358-4938-841b-1b33d7c09bae",
   "metadata": {},
   "source": [
    "# Test HDFS and Spark setup for cluster\n",
    "\n",
    "Here we try some test commands to verify the cluster setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d3dfd9d-1807-4a9b-9dce-9e601e684043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-31 09:13:11--  https://www.gutenberg.org/files/1661/1661-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 607504 (593K) [text/plain]\n",
      "Saving to: ‘/home/cluster/holmes.txt’\n",
      "\n",
      "/home/cluster/holme 100%[===================>] 593.27K  1.17MB/s    in 0.5s    \n",
      "\n",
      "2024-05-31 09:13:12 (1.17 MB/s) - ‘/home/cluster/holmes.txt’ saved [607504/607504]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download some dummy data.\n",
    "!wget https://www.gutenberg.org/files/1661/1661-0.txt -O ~/holmes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f15520-432d-4b7b-8b91-d24aeeae9299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg eBook of The Adventures of Sherlock Holmes,\n",
      "by Arthur Conan Doyle\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n"
     ]
    }
   ],
   "source": [
    "# Verify the download.\n",
    "!head -n 5 /home/cluster/holmes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b121719-09c7-49fb-b0fa-554af89da97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dummy data to hdfs.\n",
    "!hdfs dfs -mkdir /test\n",
    "!hdfs dfs -put ~/holmes.txt /test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1a752a5-3f0a-44b2-84f6-c451c18563ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   2 cluster supergroup     607504 2024-05-31 09:19 /test/holmes.txt\n"
     ]
    }
   ],
   "source": [
    "# Verify upload.\n",
    "!hdfs dfs -ls /test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37050fb5-3960-473f-92f0-f16b8d6475ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12306\n"
     ]
    }
   ],
   "source": [
    "# Read dummy data from hdfs.\n",
    "!hdfs dfs -cat /test/holmes.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ecd6d4-d074-45fb-8982-9ea82b5083c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Spark to be used by pyspark.\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Init SparkSession.\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Test Spark and HDFS\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08f16f0e-ea6f-469f-a876-aa0cfac817d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     2|\n",
      "|     4|\n",
      "|     6|\n",
      "|     8|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Try out plain pyspark.\n",
    "myRange = spark.range(1000).toDF(\"number\")\n",
    "myRange.where(\"number % 2 = 0\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d216a5db-0eb5-4925-8979-49b43dbad8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dummy data from hdfs.\n",
    "holmes_raw = spark.read.text(\"/test/holmes.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b36831ce-4014-487e-8602-912de874e72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 5709|\n",
      "| and| 2878|\n",
      "|  of| 2759|\n",
      "|  to| 2721|\n",
      "|   a| 2648|\n",
      "|   i| 2533|\n",
      "|  in| 1761|\n",
      "|that| 1604|\n",
      "| was| 1371|\n",
      "|  he| 1278|\n",
      "|  it| 1267|\n",
      "| you| 1176|\n",
      "| his| 1146|\n",
      "|  is| 1079|\n",
      "|  my|  955|\n",
      "|have|  903|\n",
      "|with|  869|\n",
      "|  as|  848|\n",
      "| had|  813|\n",
      "|  at|  768|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Simple world count example for dummy data.\n",
    "\n",
    "from pyspark.sql.functions import split, col\n",
    "wc = holmes_raw \\\n",
    "    .select(split(col(\"value\"), \" \").alias(\"sentence\")) \\\n",
    "    .selectExpr(\"(explode(sentence)) as word\") \\\n",
    "    .selectExpr(\"lower(word) as word\") \\\n",
    "    .filter(\"word != ''\") \\\n",
    "    .groupBy(\"word\") \\\n",
    "    .count()\\\n",
    "    .orderBy(\"count\", ascending=False) \\\n",
    "    .show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7ce3856-2aba-4b2e-a563-ed1968952624",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4553318-71b7-4423-9182-ebf13b78d0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /test/holmes.txt\n"
     ]
    }
   ],
   "source": [
    "# Remove dummy data.\n",
    "!hdfs dfs -rm /test/holmes.txt\n",
    "!hdfs dfs -rmdir /test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
